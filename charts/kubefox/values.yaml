certmanager:
  # TODO: HA
  #       Limit operator only namespaces owned by KF
  installCRDs: true

  prometheus:
    servicemonitor:
      enabled: true

  issuers: []

dashboards:
  enabled: false
  opensearchHosts: "https://kubefox-opensearch-main:9200"

fluentbit:
  # TODO: We only want logs from our services and units
  config:
    service: |
      [SERVICE]
          Daemon Off
          Flush {{ .Values.flush }}
          Log_Level {{ .Values.logLevel }}
          Parsers_File parsers.conf
          Parsers_File custom_parsers.conf
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port {{ .Values.metricsPort }}
          Health_Check On
          storage.path /var/log/fluentbit-buffer
    ## https://docs.fluentbit.io/manual/pipeline/inputs
    inputs: |
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          multiline.parser docker, cri
          Tag kube.*
          Mem_Buf_Limit 5MB
          Refresh_Interval 15
          Skip_Long_Lines On
          storage.type filesystem
    ## https://docs.fluentbit.io/manual/pipeline/filters
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Merge_Log On
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On
      [FILTER]
          Name modify
          Match *
          Remove ts
          Remove time
          Rename log msg
          Rename message msg
    ## https://docs.fluentbit.io/manual/pipeline/outputs
    outputs: |
      [OUTPUT]
          Name es
          Match kube.*
          Host kubefox-opensearch-main
          tls On
          tls.verify Off
          net.keepalive off
          # TODO: You know, real security
          HTTP_User admin
          HTTP_Passwd admin
          Logstash_Format On
          Logstash_Prefix kubefox-logs
          # Replace_Dots fixes issus where labels can be string or object with same key.
          # Before: kubernetes.labels.app vs kubernetes.labels.app.kubernetes.io/name
          # After:  kubernetes.labels.app vs kubernetes.labels.app_kubernetes_io/name
          Replace_Dots On
          Retry_Limit 10
          # Type name is deprecated, turn it off
          Suppress_Type_Name On
          Trace_Error On
          # Trace_Output On

nats:
  nats:
    jetstream:
      enabled: true
      memStorage:
        enabled: true
        size: 2Gi
      fileStorage:
        enabled: true
        size: 64Gi
        # storageClassName: ssd

  cluster:
    enabled: true
    replicas: 3

  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/instance
                operator: In
                values:
                  - "{{ .Release.Name }}"
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - '{{ include "kubefox.name" . }}'
          topologyKey: kubernetes.io/hostname

  natsbox:
    enabled: false

opensearch:
  clusterName: kubefox-opensearch
  nodeGroup: main
  masterService: kubefox-opensearch-main

  replicas: 3

  antiAffinity: "hard"

  persistence:
    size: 128Gi
    # storageClass: hdd
    accessModes:
      - ReadWriteOnce

  resources:
    requests:
      cpu: "500m"
      memory: "1024Mi"

  # TODO: Probably move this into our chart with a default domain etc, gives us more control
  #       Do we even want ingress, these are internal, can proxy during dev
  # ingress:
  #   enabled: true
  #   # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
  #   # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
  #   ingressClassName: traefik

  #   annotations:
  #     cert-manager.io/cluster-issuer: letsencrypt
  #     # kubernetes.io/ingress.class: nginx
  #     # kubernetes.io/tls-acme: "true"
  #   path: /
  #   hosts:
  #   - opensearch.kubefox.io
  #   # tls:
  #   # - secretName: opensearch-cert
  #   #   hosts:
  #   #   - opensearch.kubefox.io

  config:
    # TODO: This needs to be refined, especially for security
    # Values must be YAML literal style scalar / YAML multiline string.
    # <filename>: |
    #   <formatted-value(s)>
    # log4j2.properties: |
    #   status = error
    #
    #   appender.console.type = Console
    #   appender.console.name = console
    #   appender.console.layout.type = PatternLayout
    #   appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
    #
    #   rootLogger.level = info
    #   rootLogger.appenderRef.console.ref = console
    opensearch.yml: |
      cluster.name: opensearch-cluster

      # Bind to all interfaces because we don't know what IP address Docker will assign to us.
      network.host: 0.0.0.0

      # Needed to support Elasticsearch beats
      # https://www.opensearch.org/docs/latest/clients/agents-and-ingestion-tools/index/
      compatibility.override_main_response_version: true

      # # minimum_master_nodes need to be explicitly set when bound on a public IP
      # # set to 1 to allow single node clusters
      # discovery.zen.minimum_master_nodes: 1

      # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
      # discovery.type: single-node

      # Start OpenSearch Security Demo Configuration
      # WARNING: revise all the lines below before you go into production
      plugins:
        security:
          ssl:
            transport:
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
              enforce_hostname_verification: false
            http:
              enabled: true
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
          allow_unsafe_democertificates: true
          allow_default_init_securityindex: true
          authcz:
            admin_dn:
              - CN=kirk,OU=client,O=client,L=test,C=de
          audit.type: internal_opensearch
          enable_snapshot_restore_privilege: true
          check_snapshot_restore_write_privileges: true
          restapi:
            roles_enabled: ["all_access", "security_rest_api_access"]
          system_indices:
            enabled: true
            indices:
              [
                ".opendistro-alerting-config",
                ".opendistro-alerting-alert*",
                ".opendistro-anomaly-results*",
                ".opendistro-anomaly-detector*",
                ".opendistro-anomaly-checkpoints",
                ".opendistro-anomaly-detection-state",
                ".opendistro-reports-*",
                ".opendistro-notifications-*",
                ".opendistro-notebooks",
                ".opendistro-asynchronous-search-response*",
              ]

prometheus:
  prometheusOperator:
  # logFormat: json-formatted

  prometheus:
    prometheusSpec:
      # TODO: We only want metrics from our services and units

      # logFormat: json-formatted
      retention: 1d

      remoteWrite:
        - url: "http://localhost:9201/write"

      # Mount configs needed by MetricBeat sidecar
      volumes:
        - name: metricbeat-config
          configMap:
            name: metricbeat

      # Add MetricBeat sidecar to proxy metrics to OpenSearch
      containers:
        - name: metricbeat
          image: docker.elastic.co/beats/metricbeat-oss:7.12.1
          args:
            - "-c"
            - "/etc/metricbeat.yml"
            - "-e"
          env:
            - name: ELASTICSEARCH_HOST
              value: https://kubefox-opensearch-main
            - name: ELASTICSEARCH_PORT
              value: "9200"
            # TODO: Integrate creds with vault
            - name: ELASTICSEARCH_USERNAME
              value: admin
            - name: ELASTICSEARCH_PASSWORD
              value: admin
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
          volumeMounts:
            - name: metricbeat-config
              subPath: metricbeat.yml
              mountPath: /etc/metricbeat.yml
              readOnly: true
            - name: metricbeat-config
              subPath: modules.yml
              mountPath: /usr/share/metricbeat/modules.d/modules.yml
              readOnly: true

      storageSpec:
        volumeClaimTemplate:
          spec:
            # storageClassName: ssd
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 8Gi

      enableFeatures: []
      # TODO: should be enabled at some point as we just remote write to opensearch
      # https://prometheus.io/docs/prometheus/latest/feature_flags/#prometheus-agent
      # - agent

  metricbeat:
    config: |-
      metricbeat.config.modules:
        path: ${path.config}/modules.d/*.yml
        reload.enabled: true
      setup.template:
        name: kubefox-metrics
        pattern: kubefox-metrics-*
      output.elasticsearch:
        hosts: ['${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}']
        username: ${ELASTICSEARCH_USERNAME}
        password: ${ELASTICSEARCH_PASSWORD}
        index: kubefox-metrics-%{+yyyy.MM.dd}
        ssl.verification_mode: none
    modules: |-
      - module: prometheus
        metricsets: ["remote_write"]
        host: "localhost"
        port: "9201"

  defaultRules:
    create: false
  alertmanager:
    enabled: false
  grafana:
    enabled: false
  kubeApiServer:
    enabled: false
  kubelet:
    enabled: false
  kubeControllerManager:
    enabled: false
  coreDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false
  kubeStateMetrics:
    enabled: false

traefik:
  deployment:
    replicas: 3

  # https://github.com/traefik/traefik-helm-chart/issues/468
  # affinity:
  #   podAntiAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       - labelSelector:
  #           matchExpressions:
  #             - key: app.kubernetes.io/instance
  #               operator: In
  #               values:
  #                 - "{{ .Release.Name }}"
  #             - key: app.kubernetes.io/name
  #               operator: In
  #               values:
  #                 - '{{ include "kubefox.name" . }}'
  #         topologyKey: kubernetes.io/hostname

  # TODO: Do we really want to even enable an ingress class?
  #       Perhaps better to just use Traefik IngressRoute CRD internally?
  #       https://doc.traefik.io/traefik/providers/kubernetes-crd/
  #       Gives us more control and nicer matching rule language
  ingressClass:
    enabled: false # TODO: make true, fails template cause of --api-versions
    isDefaultClass: false

  providers:
    # TODO: We only want ingress from our services and units
    kubernetesCRD:
      enabled: true
      allowCrossNamespace: false
      allowExternalNameServices: false
      # ingressClass: traefik-internal
      # labelSelector: environment=production,method=traefik
      namespaces: []

    # TODO: See "ingressClass" note above
    kubernetesIngress:
      enabled: false
      allowExternalNameServices: false
      # labelSelector: environment=production,method=traefik
      namespaces: []

  ingressRoute:
    dashboard:
      enabled: false

  ports:
    web:
      port: 8000
      expose: true
      exposedPort: 80
      protocol: TCP
      redirectTo: websecure
    websecure:
      port: 8443
      expose: true
      exposedPort: 443
      protocol: TCP
      tls:
        enabled: true

  # Disables sending telemetry to Traefik
  globalArguments: []
  # Is this needed?
  additionalArguments:
    - --pilot.dashboard=false

  logs:
    general:
      # By default, the logs use a text format (common), but you can
      # also ask for the json format in the format option
      # format: json

      # By default, the level is set to ERROR. Alternative logging levels are DEBUG, PANIC, FATAL, ERROR, WARN, and INFO.
      level: ERROR
    access:
      # To enable access logs
      enabled: false
      # By default, logs are written using the Common Log Format (CLF).
      # To write logs in JSON, use json in the format option.
      # If the given format is unsupported, the default (CLF) is used instead.
      # format: json

      # To write the logs in an asynchronous fashion, specify a bufferingSize option.
      # This option represents the number of log lines Traefik will keep in memory before writing
      # them to the selected output. In some cases, this option can greatly help performances.
      # bufferingSize: 100

      # Filtering https://docs.traefik.io/observability/access-logs/#filtering
      filters: {}
      # Fields
      # https://docs.traefik.io/observability/access-logs/#limiting-the-fieldsincluding-headers
      fields:
        general:
          defaultmode: keep
          names: {}
        headers:
          defaultmode: drop
          names: {}
