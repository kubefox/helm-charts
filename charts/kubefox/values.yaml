## Image References ##
######################
images:
  fluentbit: &fluentbit_image
    repository: fluent/fluent-bit
    tag: 1.8.14
  jaegerCollector: &jaeger_collector_image
    repository: jaegertracing/jaeger-collector
    tag: 1.30.0
  metricbeat: &metricbeat_image
    repository: docker.elastic.co/beats/metricbeat-oss
    tag: 7.12.1

certmanager:
  # TODO: HA
  #       Limit operator only namespaces owned by KF
  installCRDs: true

  prometheus:
    servicemonitor:
      enabled: true

  issuers: []

collector:
  enabled: true

  opensearch:
    url: https://kubefox-opensearch-main:9200
    # TODO:
    username: admin
    password: admin

  logs:
    enabled: true

    image:
      <<: *fluentbit_image

    port: 24224
    metricsPort: 2020

    config: |-
      [SERVICE]
          Daemon Off
          Flush {{ .Values.fluentbit.flush }}
          Log_Level {{ .Values.fluentbit.logLevel }}
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port {{ .Values.collector.logs.metricsPort }}
          Health_Check On
      [INPUT]
          Name              forward
          Listen            0.0.0.0
          Port              {{ .Values.collector.logs.port }}
          Buffer_Chunk_Size 1M
          Buffer_Max_Size   6M
      [OUTPUT]
          Name es
          Host kubefox-opensearch-main
          tls On
          tls.verify Off
          net.keepalive off
          # TODO: You know, real security
          HTTP_User admin
          HTTP_Passwd admin
          Logstash_Format On
          Logstash_Prefix kubefox-logs
          # Replace_Dots fixes issus where labels can be string or object with same key.
          # Before: kubernetes.labels.app vs kubernetes.labels.app.kubernetes.io/name
          # After:  kubernetes.labels.app vs kubernetes.labels.app_kubernetes_io/name
          Replace_Dots On
          Retry_Limit 10
          # Type name is deprecated, turn it off
          Suppress_Type_Name On
          Trace_Error On
          # Trace_Output On

  metrics:
    enabled: true

    image:
      <<: *metricbeat_image

    resources:
      requests:
        cpu: 100m
        memory: 128Mi

    port: 9201

    config:
      metricbeat.config.modules:
        path: ${path.config}/modules.d/*.yaml
        reload.enabled: true
      setup.template:
        name: kubefox-metrics
        pattern: kubefox-metrics-*
      output.elasticsearch:
        hosts:
          - "{{ .Values.collector.opensearch.url }}"
        username: "{{ .Values.collector.opensearch.username }}"
        password: "{{ .Values.collector.opensearch.password }}"
        index: kubefox-metrics-%{+yyyy.MM.dd}
        ssl.verification_mode: none

    modules:
      - module: prometheus
        metricsets: ["remote_write"]
        host: "0.0.0.0"
        port: "{{ .Values.collector.metrics.port }}"

  traces:
    enabled: true

    image:
      <<: *jaeger_collector_image

    port: 14250
    spanStorageType: elasticsearch

    config:
      collector:
        grpc-server:
          host-port: ":{{ .Values.collector.traces.port }}"
      span-storage:
        type: "{{ .Values.collector.traces.spanStorageType }}"
      es:
        username: "{{ .Values.collector.opensearch.username }}"
        password: "{{ .Values.collector.opensearch.password }}"
        server-urls: "{{ .Values.collector.opensearch.url }}"
        tls:
          enabled: true
          skip-host-verify: true
        index-date-separator: "."
        index-prefix: kubefox

  service:
    type: ClusterIP
    labels: {}
    annotations: {}

  deployment:
    labels: {}
    annotations: {}
    replicas: 1
    strategy: {}

  pod:
    labels: {}
    annotations: {}

    # TODO: setup service account
    imagePullSecrets: []
    securityContext: null
    nodeSelector: {}
    tolerations: {}
    affinity: {}

fluentbit:
  flush: 1
  logLevel: info

  image:
    <<: *fluentbit_image

  # TODO: We only want logs from our services and units
  config:
    service: |
      [SERVICE]
          Daemon Off
          Flush {{ .Values.flush }}
          Log_Level {{ .Values.logLevel }}
          Parsers_File parsers.conf
          Parsers_File custom_parsers.conf
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port {{ .Values.metricsPort }}
          Health_Check On
          storage.path /var/log/fluentbit-buffer
    ## https://docs.fluentbit.io/manual/pipeline/inputs
    inputs: |
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          multiline.parser docker, cri
          Tag kube.*
          Mem_Buf_Limit 5MB
          Refresh_Interval 15
          Skip_Long_Lines On
          storage.type filesystem
    ## https://docs.fluentbit.io/manual/pipeline/filters
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Merge_Log On
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On
      [FILTER]
          Name modify
          Match *
          Remove ts
          Remove time
          Rename log msg
          Rename message msg
    ## https://docs.fluentbit.io/manual/pipeline/outputs
    outputs: |
      [OUTPUT]
          Name forward
          Match kube.*
          Host kubefox-collector
          Port 24224

jaeger:
  provisionDataStore:
    cassandra: false
    elasticsearch: false
    kafka: false

  storage:
    type: elasticsearch
    elasticsearch:
      scheme: https
      host: kubefox-opensearch-main
      port: 9200
      user: admin
      password: admin
      cmdlineParams:
        es.index-prefix: kubefox
        es.index-date-separator: "."
        es.tls.enabled: true
        es.tls.skip-host-verify: true

  agent:
    cmdlineParams:
      reporter.grpc.host-port: kubefox-collector:14250

    daemonset:
      useHostPort: true

  collector:
    enabled: false

  query:
    extraEnv:
      - name: JAEGER_AGENT_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.hostIP
    agentSidecar:
      enabled: false

metacontroller: {}

nats:
  nats:
    jetstream:
      enabled: true
      memStorage:
        enabled: true
        size: 2Gi
      fileStorage:
        enabled: true
        size: 64Gi
        # storageClassName: ssd

  cluster:
    enabled: true
    replicas: 3

  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/instance
                operator: In
                values:
                  - "{{ .Release.Name }}"
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - '{{ include "kubefox.name" . }}'
          topologyKey: kubernetes.io/hostname

  natsbox:
    enabled: false

opensearch:
  clusterName: kubefox-opensearch
  nodeGroup: main
  masterService: kubefox-opensearch-main

  replicas: 3

  antiAffinity: "hard"

  persistence:
    size: 128Gi
    # storageClass: hdd
    accessModes:
      - ReadWriteOnce

  resources:
    requests:
      cpu: "500m"
      memory: "1024Mi"

  # TODO: Probably move this into our chart with a default domain etc, gives us more control
  #       Do we even want ingress, these are internal, can proxy during dev
  # ingress:
  #   enabled: true
  #   # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
  #   # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
  #   ingressClassName: traefik

  #   annotations:
  #     cert-manager.io/cluster-issuer: letsencrypt
  #     # kubernetes.io/ingress.class: nginx
  #     # kubernetes.io/tls-acme: "true"
  #   path: /
  #   hosts:
  #   - opensearch.kubefox.io
  #   # tls:
  #   # - secretName: opensearch-cert
  #   #   hosts:
  #   #   - opensearch.kubefox.io

  config:
    # TODO: This needs to be refined, especially for security
    # Values must be YAML literal style scalar / YAML multiline string.
    # <filename>: |
    #   <formatted-value(s)>
    # log4j2.properties: |
    #   status = error
    #
    #   appender.console.type = Console
    #   appender.console.name = console
    #   appender.console.layout.type = PatternLayout
    #   appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
    #
    #   rootLogger.level = info
    #   rootLogger.appenderRef.console.ref = console
    opensearch.yml: |
      cluster.name: opensearch-cluster

      # Bind to all interfaces because we don't know what IP address Docker will assign to us.
      network.host: 0.0.0.0

      # Needed to support Elasticsearch beats
      # https://www.opensearch.org/docs/latest/clients/agents-and-ingestion-tools/index/
      compatibility.override_main_response_version: true

      # # minimum_master_nodes need to be explicitly set when bound on a public IP
      # # set to 1 to allow single node clusters
      # discovery.zen.minimum_master_nodes: 1

      # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
      # discovery.type: single-node

      # Start OpenSearch Security Demo Configuration
      # WARNING: revise all the lines below before you go into production
      plugins:
        security:
          ssl:
            transport:
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
              enforce_hostname_verification: false
            http:
              enabled: true
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
          allow_unsafe_democertificates: true
          allow_default_init_securityindex: true
          authcz:
            admin_dn:
              - CN=kirk,OU=client,O=client,L=test,C=de
          audit.type: internal_opensearch
          enable_snapshot_restore_privilege: true
          check_snapshot_restore_write_privileges: true
          restapi:
            roles_enabled: ["all_access", "security_rest_api_access"]
          system_indices:
            enabled: true
            indices:
              [
                ".opendistro-alerting-config",
                ".opendistro-alerting-alert*",
                ".opendistro-anomaly-results*",
                ".opendistro-anomaly-detector*",
                ".opendistro-anomaly-checkpoints",
                ".opendistro-anomaly-detection-state",
                ".opendistro-reports-*",
                ".opendistro-notifications-*",
                ".opendistro-notebooks",
                ".opendistro-asynchronous-search-response*",
              ]

prometheus:
  prometheusOperator:
  # logFormat: json-formatted

  prometheus:
    prometheusSpec:
      # TODO: We only want metrics from our services and units

      # logFormat: json-formatted
      retention: 1d

      remoteWrite:
        - url: "http://kubefox-collector:9201/write"

      storageSpec:
        volumeClaimTemplate:
          spec:
            # storageClassName: ssd
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 8Gi

      enableFeatures: []
      # TODO: should be enabled at some point as we just remote write to opensearch
      # https://prometheus.io/docs/prometheus/latest/feature_flags/#prometheus-agent
      # - agent

  defaultRules:
    create: false
  alertmanager:
    enabled: false
  grafana:
    enabled: false
  kubeApiServer:
    enabled: false
  kubelet:
    enabled: false
  kubeControllerManager:
    enabled: false
  coreDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false
  kubeStateMetrics:
    enabled: false

traefik:
  deployment:
    replicas: 3

  # https://github.com/traefik/traefik-helm-chart/issues/468
  # affinity:
  #   podAntiAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       - labelSelector:
  #           matchExpressions:
  #             - key: app.kubernetes.io/instance
  #               operator: In
  #               values:
  #                 - "{{ .Release.Name }}"
  #             - key: app.kubernetes.io/name
  #               operator: In
  #               values:
  #                 - '{{ include "kubefox.name" . }}'
  #         topologyKey: kubernetes.io/hostname

  # TODO: Do we really want to even enable an ingress class?
  #       Perhaps better to just use Traefik IngressRoute CRD internally?
  #       https://doc.traefik.io/traefik/providers/kubernetes-crd/
  #       Gives us more control and nicer matching rule language
  ingressClass:
    enabled: false # TODO: make true, fails template cause of --api-versions
    isDefaultClass: false

  providers:
    # TODO: We only want ingress from our services and units
    kubernetesCRD:
      enabled: true
      allowCrossNamespace: false
      allowExternalNameServices: false
      # ingressClass: traefik-internal
      # labelSelector: environment=production,method=traefik
      namespaces: []

    # TODO: See "ingressClass" note above
    kubernetesIngress:
      enabled: false
      allowExternalNameServices: false
      # labelSelector: environment=production,method=traefik
      namespaces: []

  ingressRoute:
    dashboard:
      enabled: false

  ports:
    web:
      port: 8000
      expose: true
      exposedPort: 80
      protocol: TCP
      redirectTo: websecure
    websecure:
      port: 8443
      expose: true
      exposedPort: 443
      protocol: TCP
      tls:
        enabled: true

  env:
    - name: JAEGER_AGENT_HOST
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP

  # Disables sending telemetry to Traefik
  globalArguments: []
  # Is this needed?
  additionalArguments:
    - --pilot.dashboard=false
    - --tracing.jaeger=true
    - --tracing.jaeger.localAgentHostPort=$(JAEGER_AGENT_HOST):6831

  logs:
    general:
      # By default, the logs use a text format (common), but you can
      # also ask for the json format in the format option
      # format: json

      # By default, the level is set to ERROR. Alternative logging levels are DEBUG, PANIC, FATAL, ERROR, WARN, and INFO.
      level: ERROR
    access:
      # To enable access logs
      enabled: false
      # By default, logs are written using the Common Log Format (CLF).
      # To write logs in JSON, use json in the format option.
      # If the given format is unsupported, the default (CLF) is used instead.
      # format: json

      # To write the logs in an asynchronous fashion, specify a bufferingSize option.
      # This option represents the number of log lines Traefik will keep in memory before writing
      # them to the selected output. In some cases, this option can greatly help performances.
      # bufferingSize: 100

      # Filtering https://docs.traefik.io/observability/access-logs/#filtering
      filters: {}
      # Fields
      # https://docs.traefik.io/observability/access-logs/#limiting-the-fieldsincluding-headers
      fields:
        general:
          defaultmode: keep
          names: {}
        headers:
          defaultmode: drop
          names: {}
